// üì¶ IMPORTS - Bringing in code from other files and libraries

// React core - the main library for building user interfaces
// useState: Hook for managing component state (data that can change)
// useEffect: Hook for side effects (cleanup, API calls, etc.)
// useRef: Hook for accessing DOM elements or storing mutable values
import React, { useState, useEffect, useRef } from 'react';

// Lucide React - beautiful icons for our buttons
// Mic: Microphone icon for recording state
// MicOff: Microphone off icon for stopped state
import { Mic, MicOff } from 'lucide-react';

// Our custom CSS styles for making the app look good
import './App.css';

// TypeScript type definitions - these help prevent bugs by defining data shapes
// These interfaces define the structure of data we expect to receive/send
import { TranscriptionMessage, AISummary, ConnectionStatus, SummaryType } from './types';

/**
 * üé§ MAIN APP COMPONENT
 * 
 * This is the main React component that handles:
 * - Recording audio from the user's microphone
 * - Sending audio data to the backend via WebSocket
 * - Receiving real-time transcription from Deepgram
 * - Generating AI summaries using Google Gemini
 * - Managing all the UI state and user interactions
 * 
 * FLOW:
 * 1. User clicks "Start Recording"
 * 2. App requests microphone permission
 * 3. App connects to WebSocket backend
 * 4. Audio is streamed to backend ‚Üí Deepgram ‚Üí transcription appears
 * 5. User clicks "Stop & Summarize"
 * 6. App automatically generates AI summary
 */
function App() {
  console.log('üöÄ App component rendering/re-rendering');

  // üîÑ STATE MANAGEMENT
  // React hooks for managing component state - these variables can change and trigger re-renders
  
  // üé§ RECORDING STATE: tracks whether we're currently recording audio
  // boolean = true/false value
  const [isRecording, setIsRecording] = useState<boolean>(false);
  
  // üìù TRANSCRIPTION TEXT: holds the final transcribed text from Deepgram
  // string = text value that gets displayed in the main text box
  const [transcription, setTranscription] = useState<string>('');
  
  // ‚è±Ô∏è INTERIM TEXT: temporary text being processed (live feedback)
  // This shows what's being transcribed in real-time before it's finalized
  const [interimText, setInterimText] = useState<string>('');
  
  // üîó CONNECTION STATUS: tracks connection to backend WebSocket
  // ConnectionStatus is a custom type defined in types.ts
  const [connectionStatus, setConnectionStatus] = useState<ConnectionStatus>('Disconnected');
  
  // ‚ö†Ô∏è ERROR MESSAGES: for showing errors to user when things go wrong
  const [error, setError] = useState<string>('');
  
  // ü§ñ AI SUMMARY: holds the generated summary from Google Gemini
  // AISummary | null means it can be a summary object OR null (empty)
  const [aiSummary, setAiSummary] = useState<AISummary | null>(null);
  
  // ‚è≥ LOADING STATE: shows spinner/loading text when generating summary
  const [isGeneratingSummary, setIsGeneratingSummary] = useState<boolean>(false);

  // üìä LOG CURRENT STATE VALUES
  console.log('üìä Current State Values:', {
    isRecording,
    transcriptionLength: transcription.length,
    interimTextLength: interimText.length,
    connectionStatus,
    hasError: !!error,
    errorMessage: error,
    hasAiSummary: !!aiSummary,
    isGeneratingSummary
  });

  // üîó REFS FOR PERSISTENT OBJECTS
  // useRef creates references to objects that persist across re-renders
  // These don't trigger re-renders when changed (unlike useState)
  // 
  // WHY USE useRef INSTEAD OF useState?
  // - useState: For data that affects what the user sees (UI state)
  // - useRef: For "utility objects" that work behind the scenes
  // 
  // Think of useRef like a "storage box" that:
  // ‚úÖ Keeps the same object between component re-renders
  // ‚úÖ Doesn't cause re-renders when the object changes
  // ‚úÖ Perfect for APIs, connections, and DOM references
  
  // üåê WEBSOCKET CONNECTION REF
  // WebSocket is a built-in browser API (no import needed) for real-time communication
  // 
  // WHAT IT DOES:
  // - Creates a persistent connection to the Python backend server
  // - Allows sending audio data TO the server instantly
  // - Allows receiving transcription data FROM the server instantly
  // - Like a phone line that stays open during the entire recording session
  // 
  // LIFECYCLE:
  // 1. null (initially - no connection)
  // 2. WebSocket object (when connected to ws://localhost:8000/ws)
  // 3. null again (when connection is closed)
  const websocketRef = useRef<WebSocket | null>(null);
  
  // üéôÔ∏è MEDIARECORDER REF
  // MediaRecorder is a built-in browser API (no import needed) for recording audio/video
  // 
  // WHAT IT DOES:
  // - Takes audio from your microphone (MediaStream)
  // - Converts it into compressed audio data (WebM format with Opus codec)
  // - Splits recording into small chunks (every 100ms)
  // - Triggers events when each chunk is ready to send
  // 
  // SIMPLE ANALOGY:
  // - Your microphone = Raw sound waves
  // - MediaRecorder = Digital tape recorder that processes and packages the sound
  // - Audio chunks = Small pieces of recorded audio sent continuously
  // 
  // LIFECYCLE:
  // 1. null (initially - no recorder)
  // 2. MediaRecorder object (when recording starts)
  // 3. null again (when recording stops and cleanup happens)
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  
  // üîä AUDIO STREAM REF
  // MediaStream is a built-in browser API (no import needed) representing live audio/video
  // 
  // WHAT IT IS:
  // - The actual "live feed" from your microphone
  // - Contains audio tracks (channels of sound data)
  // - Like a "pipe" that continuously flows with audio data
  // - This is what MediaRecorder reads from to create recordings
  // 
  // SIMPLE ANALOGY:
  // - Think of water flowing through a pipe
  // - MediaStream = The pipe with audio "flowing" through it
  // - Your microphone = The source of the "audio water"
  // - MediaRecorder = A device that "collects" audio from this pipe
  // 
  // HOW WE GET IT:
  // navigator.mediaDevices.getUserMedia({ audio: true })
  // ‚Üë This asks the browser: "Give me access to the user's microphone"
  // ‚Üë Returns a MediaStream object with live audio data
  // 
  // WHY STORE IN REF:
  // - We need to stop/cleanup the stream when recording ends
  // - Stopping releases the microphone (turns off the red recording indicator)
  // - If we don't cleanup, the microphone stays "on" even after stopping
  // 
  // LIFECYCLE:
  // 1. null (initially - no microphone access)
  // 2. MediaStream object (when getUserMedia() succeeds)
  // 3. null again (when we stop all tracks and cleanup)
  const audioStreamRef = useRef<MediaStream | null>(null);

  // üîç LOG REF STATUS
  console.log('üîó Refs Status:', {
    hasWebSocket: !!websocketRef.current,
    websocketReadyState: websocketRef.current?.readyState,
    websocketReadyStateText: websocketRef.current ? 
      ['CONNECTING', 'OPEN', 'CLOSING', 'CLOSED'][websocketRef.current.readyState] : 'null',
    hasMediaRecorder: !!mediaRecorderRef.current,
    mediaRecorderState: mediaRecorderRef.current?.state,
    hasAudioStream: !!audioStreamRef.current,
    audioStreamActive: audioStreamRef.current?.active,
    audioTrackCount: audioStreamRef.current?.getTracks().length || 0
  });

  // üßπ CLEANUP EFFECT
  // useEffect with empty dependency array [] runs once when component mounts
  // The return function runs when component unmounts (cleanup)
  useEffect(() => {
    console.log('üèóÔ∏è App component mounted - setting up cleanup effect');
    
    return () => {
      console.log('üßπ App component unmounting - running cleanup');
      stopRecording();
      if (websocketRef.current) {
        console.log('üîå Closing WebSocket during cleanup');
        websocketRef.current.close();
      }
    };
  }, []); // Empty array means this effect runs only once

  // üîç STATE CHANGE EFFECT - Log when important state changes
  useEffect(() => {
    console.log('üîÑ State Change Detected:', {
      isRecording,
      connectionStatus,
      transcriptionLength: transcription.length,
      hasError: !!error
    });
  }, [isRecording, connectionStatus, transcription, error]);

  /**
   * üåê WEBSOCKET CONNECTION FUNCTION
   * 
   * Establishes real-time connection to Python backend
   * WebSocket allows bidirectional communication (unlike HTTP requests)
   * 
   * PROCESS:
   * 1. Create WebSocket connection to ws://localhost:8000/ws
   * 2. Set up event handlers for open, message, close, error
   * 3. Handle incoming transcription messages from Deepgram
   * 4. Update UI state based on messages received
   * 
   * @returns Promise that resolves when connection is established
   */
  const connectWebSocket = (): Promise<void> => {
    console.log('üåê ==================== WEBSOCKET CONNECTION START ====================');
    console.log('üåê connectWebSocket called');
    console.log('üìä WebSocket Info: What is WebSocket?');
    console.log('   ‚Ä¢ WebSocket = Persistent, bidirectional connection (like a phone call)');
    console.log('   ‚Ä¢ HTTP = One-time request/response (like sending a letter)');
    console.log('   ‚Ä¢ WebSocket stays open for real-time communication');
    console.log('   ‚Ä¢ Perfect for live audio streaming and instant transcription');
    
    return new Promise((resolve, reject) => {
      try {
        console.log('üîÑ Setting connection status to Connecting');
        console.log('üìä UI State Update: connectionStatus = "Connecting"');
        // Update UI to show we're trying to connect
        setConnectionStatus('Connecting');
        
        console.log('üîó Creating WebSocket connection to ws://localhost:8000/ws');
        console.log('üìä WebSocket URL Breakdown:');
        console.log('   ‚Ä¢ Protocol: ws:// (WebSocket, not http://)');
        console.log('   ‚Ä¢ Host: localhost (same computer)');
        console.log('   ‚Ä¢ Port: 8000 (where Python backend is running)');
        console.log('   ‚Ä¢ Path: /ws (WebSocket endpoint on backend)');
        
        // Create new WebSocket connection to backend
        const ws = new WebSocket('ws://localhost:8000/ws');
        console.log('‚úÖ WebSocket object created');
        console.log('üìä Initial WebSocket state:', {
          readyState: ws.readyState,
          readyStateText: ['CONNECTING', 'OPEN', 'CLOSING', 'CLOSED'][ws.readyState],
          url: ws.url,
          protocol: ws.protocol,
          extensions: ws.extensions
        });
        
        websocketRef.current = ws;
        console.log('üì¶ WebSocket stored in ref for later use');

        // üéâ CONNECTION OPENED - Backend is ready to receive audio
        ws.onopen = () => {
          console.log('üéâ ==================== WEBSOCKET OPENED ====================');
          console.log('‚úÖ WebSocket connected successfully!');
          console.log('üìä Connection Details:');
          console.log('   ‚Ä¢ Client (this app) ‚ÜîÔ∏è Server (Python backend) connection established');
          console.log('   ‚Ä¢ Can now send audio data TO server');
          console.log('   ‚Ä¢ Can now receive transcription FROM server');
          console.log('   ‚Ä¢ Connection is persistent (stays open until closed)');
          console.log('üìä WebSocket State After Opening:', {
            readyState: ws.readyState,
            readyStateText: ['CONNECTING', 'OPEN', 'CLOSING', 'CLOSED'][ws.readyState],
            bufferedAmount: ws.bufferedAmount
          });
          
          console.log('üîÑ Updating UI: connectionStatus = "Connected"');
          setConnectionStatus('Connected');
          setError(''); // Clear any previous errors
          console.log('üîÑ Clearing previous errors and resolving promise');
          console.log('‚úÖ Promise resolved - connection ready for use!');
          resolve(); // Promise succeeds - connection established
        };

        // üì® MESSAGE RECEIVED - Backend sent us transcription data
        ws.onmessage = (event) => {
          console.log('üì® ==================== WEBSOCKET MESSAGE RECEIVED ====================');
          console.log('üì® WebSocket message received from server!');
          console.log('üìä Raw Message Details:', {
            dataType: typeof event.data,
            dataSize: event.data.length,
            timestamp: new Date().toISOString(),
            rawData: event.data
          });
          console.log('üìä Message Flow: Server ‚Üí Client (this app)');
          console.log('   ‚Ä¢ Server processed audio and generated transcription');
          console.log('   ‚Ä¢ Server sent result back through WebSocket');
          console.log('   ‚Ä¢ This event handler receives and processes the message');
          
          try {
            console.log('üîÑ Parsing JSON message from backend...');
            // Parse JSON message from backend
            const data: TranscriptionMessage = JSON.parse(event.data);
            console.log('‚úÖ JSON parsing successful!');
            console.log('üìä Parsed message data:', data);
            console.log('üìä Message Structure Analysis:', {
              messageType: data.type,
              hasText: !!data.text,
              isFinal: data.is_final,
              hasFullTranscript: !!data.full_transcript,
              hasMessage: !!data.message
            });
            
            // Handle different types of messages
            console.log('üîÑ Processing message based on type...');
            switch (data.type) {
              case 'transcription':
                console.log('üìù ==================== TRANSCRIPTION MESSAGE ====================');
                console.log('üìù Processing transcription message from Deepgram!');
                console.log('üìä Transcription Details:', {
                  text: data.text,
                  is_final: data.is_final,
                  full_transcript_length: data.full_transcript?.length || 0,
                  textPreview: data.text ? data.text.substring(0, 50) + '...' : 'No text'
                });
                console.log('üìä Transcription Flow:');
                console.log('   ‚Ä¢ Your voice ‚Üí Microphone ‚Üí MediaRecorder ‚Üí WebSocket ‚Üí Server');
                console.log('   ‚Ä¢ Server ‚Üí Deepgram AI ‚Üí Transcription ‚Üí WebSocket ‚Üí This app');
                
                // This is transcribed text from Deepgram
                if (data.text) {
                  if (data.is_final) {
                    // Final text - update main transcription
                    console.log('‚úÖ ==================== FINAL TRANSCRIPTION ====================');
                    console.log('‚úÖ Final transcription received - this is the confirmed text!');
                    console.log('üìä Final Text Details:', {
                      finalText: data.text,
                      fullTranscriptLength: data.full_transcript?.length || 0,
                      fullTranscriptPreview: data.full_transcript ? data.full_transcript.substring(0, 100) + '...' : 'No full transcript'
                    });
                    console.log('üîÑ Updating main transcription state...');
                    setTranscription(data.full_transcript || '');
                    setInterimText(''); // Clear interim text
                    console.log('üîÑ Cleared interim text (no longer needed)');
                    console.log('‚úÖ UI updated with final transcription!');
                  } else {
                    // Interim text - show what's being processed
                    console.log('‚è±Ô∏è ==================== INTERIM TRANSCRIPTION ====================');
                    console.log('‚è±Ô∏è Interim transcription - live preview while speaking!');
                    console.log('üìä Interim Details:', {
                      interimText: data.text,
                      textLength: data.text.length,
                      isTemporary: true
                    });
                    console.log('üìä Interim vs Final:');
                    console.log('   ‚Ä¢ Interim = Live preview (may change as you continue speaking)');
                    console.log('   ‚Ä¢ Final = Confirmed text (won\'t change anymore)');
                    console.log('üîÑ Updating interim text state...');
                    setInterimText(data.text);
                    console.log('‚úÖ UI updated with interim transcription!');
                  }
                } else {
                  console.log('‚ö†Ô∏è Transcription message received but no text content');
                }
                break;
              
              case 'connection_status':
                console.log('üîó ==================== CONNECTION STATUS MESSAGE ====================');
                console.log('üîó Connection status update from server:', data.message);
                console.log('üìä Status Details:', {
                  newStatus: data.message,
                  timestamp: new Date().toISOString()
                });
                console.log('üìä Status Flow: Server monitoring ‚Üí Status change ‚Üí WebSocket ‚Üí UI update');
                // Backend is telling us about connection status
                if (data.message) {
                  console.log('üîÑ Updating connection status in UI...');
                  setConnectionStatus(data.message as ConnectionStatus);
                  console.log('‚úÖ Connection status updated!');
                }
                break;
              
              case 'error':
                console.log('‚ùå ==================== ERROR MESSAGE ====================');
                console.log('‚ùå Error message received from backend:', data.message);
                console.log('üìä Error Details:', {
                  errorMessage: data.message,
                  timestamp: new Date().toISOString(),
                  source: 'WebSocket Server'
                });
                console.log('üìä Error Flow: Server error ‚Üí WebSocket ‚Üí Client error handling ‚Üí UI error display');
                // Something went wrong on the backend
                console.log('üîÑ Setting error state for UI display...');
                setError(data.message || 'Unknown error occurred');
                console.log('‚úÖ Error state updated - user will see error message');
                break;
                
              default:
                console.log('‚ö†Ô∏è ==================== UNKNOWN MESSAGE TYPE ====================');
                console.log('‚ö†Ô∏è Received message with unknown type:', data.type);
                console.log('üìä Unknown Message Details:', data);
                break;
            }
            console.log('‚úÖ Message processing complete!');
          } catch (error) {
            console.error('‚ùå ==================== MESSAGE PARSING ERROR ====================');
            console.error('‚ùå Error parsing WebSocket message:', error);
            console.log('üìä Parsing Error Details:', {
              errorType: error instanceof Error ? error.name : 'Unknown',
              errorMessage: error instanceof Error ? error.message : String(error),
              rawMessageData: event.data,
              dataType: typeof event.data,
              dataLength: event.data.length
            });
            console.log('üìÑ Raw message data that failed to parse:', event.data);
            console.log('üí° Possible causes:');
            console.log('   ‚Ä¢ Server sent invalid JSON');
            console.log('   ‚Ä¢ Message format changed');
            console.log('   ‚Ä¢ Network corruption');
          }
        };

        // üîå CONNECTION CLOSED - Backend disconnected
        ws.onclose = (event) => {
          console.log('üîå ==================== WEBSOCKET CLOSED ====================');
          console.log('üîå WebSocket connection closed!');
          console.log('üìä Close Event Details:', {
            code: event.code,
            reason: event.reason,
            wasClean: event.wasClean,
            timestamp: new Date().toISOString()
          });
          console.log('üìä Close Code Meaning:');
          const closeReasons: { [key: number]: string } = {
            1000: 'Normal closure',
            1001: 'Going away',
            1002: 'Protocol error',
            1003: 'Unsupported data',
            1006: 'Abnormal closure',
            1011: 'Server error',
            1015: 'TLS handshake failure'
          };
          console.log(`   ‚Ä¢ Code ${event.code}: ${closeReasons[event.code] || 'Unknown reason'}`);
          console.log('üìä Connection Lifecycle: CONNECTING ‚Üí OPEN ‚Üí CLOSING ‚Üí CLOSED ‚úÖ');
          console.log('üîÑ Updating UI connection status to Disconnected...');
          setConnectionStatus('Disconnected');
          console.log('‚úÖ UI updated - user will see disconnected status');
        };

        // ‚ùå CONNECTION ERROR - Something went wrong
        ws.onerror = (error) => {
          console.error('‚ùå ==================== WEBSOCKET ERROR ====================');
          console.error('‚ùå WebSocket connection error occurred!');
          console.error('üìä Error Event:', error);
          console.log('üìä Common WebSocket Error Causes:');
          console.log('   ‚Ä¢ Backend server not running (most common)');
          console.log('   ‚Ä¢ Wrong URL or port number');
          console.log('   ‚Ä¢ Network connectivity issues');
          console.log('   ‚Ä¢ Firewall blocking connection');
          console.log('   ‚Ä¢ Server overloaded or crashed');
          console.log('üîÑ Setting connection status to error state...');
          setConnectionStatus('Connection Error');
          console.log('üîÑ Setting user-friendly error message...');
          setError('Connection failed. Make sure the backend is running.');
          console.log('üîÑ Rejecting promise due to WebSocket error');
          console.log('‚ùå Connection attempt failed - promise will be rejected');
          reject(error); // Promise fails - connection failed
        };

      } catch (error) {
        console.error('‚ùå Failed to create WebSocket connection:', error);
        setConnectionStatus('Connection Error');
        setError('Failed to connect to backend');
        reject(error);
      }
    });
  };

  /**
   * üéôÔ∏è START RECORDING FUNCTION
   * 
   * Initiates the voice recording process
   * 
   * PROCESS:
   * 1. Connect to WebSocket backend
   * 2. Request microphone permission from browser
   * 3. Create MediaRecorder to capture audio
   * 4. Set up audio streaming to backend
   * 5. Start recording with optimal settings
   * 
   * AUDIO SETTINGS:
   * - sampleRate: 16000 Hz (CD quality, good for speech recognition)
   * - channelCount: 1 (mono - single channel is sufficient for speech)
   * - echoCancellation: true (removes echo feedback)
   * - noiseSuppression: true (reduces background noise)
   */
  const startRecording = async () => {
    console.log('üéôÔ∏è startRecording called');
    console.log('üìä Pre-recording state:', {
      isRecording,
      connectionStatus,
      hasWebSocket: !!websocketRef.current,
      hasMediaRecorder: !!mediaRecorderRef.current,
      hasAudioStream: !!audioStreamRef.current
    });

    try {
      console.log('üîÑ Clearing previous errors');
      setError(''); // Clear any previous errors
      
      // STEP 1: Connect to WebSocket backend first
      console.log('üåê Step 1: Connecting to WebSocket backend');
      await connectWebSocket();
      console.log('‚úÖ WebSocket connection established');
      
      // STEP 2: Request microphone access from browser
      // This will show a permission dialog to the user
      console.log('üé§ Step 2: Requesting microphone access with settings:', {
        sampleRate: 16000,
        channelCount: 1,
        echoCancellation: true,
        noiseSuppression: true
      });
      
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          sampleRate: 16000,        // 16kHz sample rate (good for speech)
          channelCount: 1,          // Mono audio (single channel)
          echoCancellation: true,   // Remove echo
          noiseSuppression: true    // Reduce background noise
        } 
      });
      
      console.log('‚úÖ Microphone access granted:', {
        streamId: stream.id,
        streamActive: stream.active,
        trackCount: stream.getTracks().length,
        audioTracks: stream.getAudioTracks().map(track => ({
          id: track.id,
          label: track.label,
          enabled: track.enabled,
          readyState: track.readyState
        }))
      });
      
      // Store the audio stream for later cleanup
      audioStreamRef.current = stream;
      
      // STEP 3: Create MediaRecorder to capture and encode audio
      // WebM with Opus codec provides good compression for real-time streaming
      console.log('üé¨ Step 3: Creating MediaRecorder with mimeType: audio/webm;codecs=opus');
      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: 'audio/webm;codecs=opus'
      });
      
      console.log('üìä MediaRecorder created:', {
        state: mediaRecorder.state,
        mimeType: mediaRecorder.mimeType,
        audioBitsPerSecond: mediaRecorder.audioBitsPerSecond
      });
      
      mediaRecorderRef.current = mediaRecorder;
      
              // STEP 4: Set up audio data handler
        // This function runs every time MediaRecorder has audio data ready
        console.log('üì° Step 4: Setting up audio data handler');
        mediaRecorder.ondataavailable = (event) => {
          console.log('üì° ==================== AUDIO DATA AVAILABLE ====================');
          console.log('üì° MediaRecorder has audio data ready to send!');
          console.log('üìä Audio Data Details:', {
            dataSize: event.data.size,
            dataType: event.data.type,
            dataSizeKB: (event.data.size / 1024).toFixed(2) + ' KB',
            timestamp: new Date().toISOString(),
            websocketState: websocketRef.current?.readyState,
            websocketStateText: websocketRef.current ? 
              ['CONNECTING', 'OPEN', 'CLOSING', 'CLOSED'][websocketRef.current.readyState] : 'null',
            websocketOpen: websocketRef.current?.readyState === WebSocket.OPEN
          });
          console.log('üìä Audio Data Flow:');
          console.log('   ‚Ä¢ Microphone captures sound waves');
          console.log('   ‚Ä¢ MediaRecorder converts to digital audio (WebM/Opus)');
          console.log('   ‚Ä¢ Audio split into 100ms chunks for real-time streaming');
          console.log('   ‚Ä¢ Each chunk triggers this ondataavailable event');
          console.log('   ‚Ä¢ We send chunk through WebSocket to server for transcription');
          
          // Only send if we have data and WebSocket is connected
          if (event.data.size > 0 && websocketRef.current?.readyState === WebSocket.OPEN) {
            console.log('‚úÖ Conditions met for sending audio data:');
            console.log('   ‚úÖ Data size > 0 (' + event.data.size + ' bytes)');
            console.log('   ‚úÖ WebSocket is OPEN and ready');
            console.log('üì§ ==================== SENDING AUDIO TO SERVER ====================');
            console.log('üì§ Sending audio chunk to backend via WebSocket...');
            
            // Send raw audio data to backend for transcription
            websocketRef.current.send(event.data);
            
            console.log('‚úÖ Audio data sent successfully!');
            console.log('üìä What happens next:');
            console.log('   ‚Ä¢ Server receives audio chunk');
            console.log('   ‚Ä¢ Server forwards to Deepgram for transcription');
            console.log('   ‚Ä¢ Deepgram processes audio and returns text');
            console.log('   ‚Ä¢ Server sends transcription back via WebSocket');
            console.log('   ‚Ä¢ We receive transcription in onmessage handler');
          } else {
            console.log('‚ùå ==================== NOT SENDING AUDIO ====================');
            console.log('‚ùå Cannot send audio data - conditions not met:');
            if (event.data.size === 0) {
              console.log('   ‚ùå No audio data available (size = 0)');
              console.log('   üí° This can happen if microphone is muted or no sound detected');
            } else {
              console.log('   ‚úÖ Audio data available (' + event.data.size + ' bytes)');
            }
            
            if (websocketRef.current?.readyState !== WebSocket.OPEN) {
              console.log('   ‚ùå WebSocket not open (state: ' + 
                (websocketRef.current ? 
                  ['CONNECTING', 'OPEN', 'CLOSING', 'CLOSED'][websocketRef.current.readyState] : 'null') + ')');
              console.log('   üí° WebSocket must be OPEN to send data');
            } else {
              console.log('   ‚úÖ WebSocket is OPEN and ready');
            }
            
            console.log('‚ö†Ô∏è Audio chunk will be discarded (not sent to server)');
          }
        };
      
      // STEP 5: Start recording
      // 100ms intervals = send audio data every 100 milliseconds for real-time processing
      console.log('üé¨ Step 5: Starting MediaRecorder with 100ms intervals');
      mediaRecorder.start(100);
      
      console.log('üîÑ Setting isRecording to true');
      setIsRecording(true); // Update UI state
      
      console.log('‚úÖ Recording started successfully');
      
    } catch (error) {
      console.error('‚ùå Error starting recording:', error);
      console.log('üìä Error details:', {
        errorName: error instanceof Error ? error.name : 'Unknown',
        errorMessage: error instanceof Error ? error.message : String(error),
        errorStack: error instanceof Error ? error.stack : undefined
      });
      setError('Failed to start recording. Please check microphone permissions.');
    }
  };

  /**
   * üõë STOP RECORDING FUNCTION
   * 
   * Stops the recording and automatically generates AI summary
   * 
   * PROCESS:
   * 1. Stop MediaRecorder
   * 2. Stop and cleanup audio stream
   * 3. Close WebSocket connection
   * 4. Wait briefly for final transcription
   * 5. Automatically generate AI summary
   * 
   * AUTO-SUMMARY FEATURE:
   * After stopping, the app automatically generates a summary
   * This provides immediate value without requiring extra clicks
   */
  const stopRecording = () => {
    console.log('üõë stopRecording called');
    
    try {
      // üîç DEBUG: Log initial stopRecording state
      console.log('üõë stopRecording called', {
        isRecording,
        mediaRecorderState: mediaRecorderRef.current?.state,
        hasAudioStream: !!audioStreamRef.current,
        hasWebSocket: !!websocketRef.current,
        transcriptionLength: transcription.length,
      });
      
      // STEP 1: Stop the MediaRecorder if it's active
      if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
        console.log('üé¨ Stopping MediaRecorder (current state:', mediaRecorderRef.current.state + ')');
        mediaRecorderRef.current.stop();
        console.log('‚úÖ MediaRecorder stopped');
      } else {
        console.log('‚ö†Ô∏è MediaRecorder not active or not available:', {
          hasMediaRecorder: !!mediaRecorderRef.current,
          state: mediaRecorderRef.current?.state
        });
      }
      
      // STEP 2: Stop the audio stream and release microphone
      if (audioStreamRef.current) {
        console.log('üîä Stopping audio stream tracks');
        const tracks = audioStreamRef.current.getTracks();
        console.log('üìä Audio tracks to stop:', tracks.length);
        
        // Stop all audio tracks (releases microphone access)
        tracks.forEach((track, index) => {
          console.log(`üõë Stopping track ${index}:`, {
            id: track.id,
            label: track.label,
            readyState: track.readyState
          });
          track.stop();
        });
        
        audioStreamRef.current = null;
        console.log('‚úÖ Audio stream tracks stopped and reference cleared');
      } else {
        console.log('‚ö†Ô∏è No audio stream to stop');
      }
      
      // STEP 3: Close WebSocket connection
      if (websocketRef.current) {
        console.log('üîå ==================== CLOSING WEBSOCKET ====================');
        console.log('üîå Closing WebSocket connection...');
        console.log('üìä WebSocket State Before Closing:', {
          readyState: websocketRef.current.readyState,
          readyStateText: ['CONNECTING', 'OPEN', 'CLOSING', 'CLOSED'][websocketRef.current.readyState],
          url: websocketRef.current.url,
          bufferedAmount: websocketRef.current.bufferedAmount
        });
        console.log('üìä Why Close WebSocket?');
        console.log('   ‚Ä¢ Recording stopped - no more audio to send');
        console.log('   ‚Ä¢ Prevents unnecessary network connection');
        console.log('   ‚Ä¢ Cleans up resources properly');
        console.log('   ‚Ä¢ Server can free up connection slot');
        
        console.log('üîÑ Calling websocket.close()...');
        websocketRef.current.close();
        console.log('üîÑ Clearing WebSocket reference...');
        websocketRef.current = null;
        console.log('‚úÖ WebSocket closed and reference cleared');
        console.log('üìä Connection Lifecycle Complete: CONNECTING ‚Üí OPEN ‚Üí CLOSING ‚Üí CLOSED ‚úÖ');
      } else {
        console.log('‚ö†Ô∏è ==================== NO WEBSOCKET TO CLOSE ====================');
        console.log('‚ö†Ô∏è No WebSocket connection to close');
        console.log('üí° This can happen if:');
        console.log('   ‚Ä¢ Connection was never established');
        console.log('   ‚Ä¢ Connection already closed due to error');
        console.log('   ‚Ä¢ Multiple stop recording calls');
      }
      
      // STEP 4: Update UI state
      console.log('üîÑ Setting isRecording to false');
      setIsRecording(false);
      console.log('‚úÖ isRecording state updated to false');
      
      // STEP 5: ü§ñ AUTO-GENERATE SUMMARY
      // Wait 1 second for any final transcription to arrive, then generate summary
      console.log('‚è∞ Setting timeout for auto-summary generation (1 second)');
      setTimeout(() => {
        console.log('‚è∞ Timeout reached for auto-summary');
        console.log('üìä Current transcription length:', transcription.length);
        console.log('üìÑ Transcription preview:', transcription.substring(0, 100) + '...');
        
        if (transcription.trim()) { // Only if we have transcribed text
          console.log('ü§ñ Transcription available - generating summary automatically');
          generateSummary();
        } else {
          console.log('‚ö†Ô∏è No transcription available - skipping auto-summary');
        }
      }, 1000);
      
    } catch (error) {
      console.error('‚ùå Error stopping recording:', error);
      console.log('üìä Stop recording error details:', {
        errorName: error instanceof Error ? error.name : 'Unknown',
        errorMessage: error instanceof Error ? error.message : String(error)
      });
      setError('Error stopping recording');
    }
  };

  /**
   * ü§ñ GENERATE AI SUMMARY FUNCTION
   * 
   * Sends transcribed text to backend for AI analysis using Google Gemini
   * 
   * PROCESS:
   * 1. Validate we have text to summarize
   * 2. Send POST request to backend /api/summarize endpoint
   * 3. Backend forwards to Google Gemini AI
   * 4. Receive structured summary with key points, action items, etc.
   * 5. Display results in UI
   * 
   * SUMMARY TYPES:
   * - 'meeting': Comprehensive meeting summary (default)
   * - 'action_items': Focus on tasks and to-dos
   * - 'key_points': Main takeaways and important points
   * - 'speaker_analysis': Per-speaker breakdown (if diarization enabled)
   * 
   * @param summaryType - Type of summary to generate
   */
  const generateSummary = async (summaryType: SummaryType = 'meeting') => {
    console.log('ü§ñ generateSummary called with type:', summaryType);
    
    // Validate we have content to summarize
    if (!transcription.trim()) {
      console.log('‚ùå No transcription available to summarize');
      setError('No transcription available to summarize');
      return;
    }

    console.log('üìä Transcription validation passed:', {
      transcriptionLength: transcription.length,
      transcriptionPreview: transcription.substring(0, 200) + '...'
    });

    // Update UI to show loading state
    console.log('üîÑ Setting loading states');
    setIsGeneratingSummary(true);
    setAiSummary(null); // Clear previous summary
    setError(''); // Clear any errors
    
    // üîç DEBUG: Preparing to generate summary
    console.log('generateSummary called', {
      summaryType,
      transcriptionLength: transcription.length,
    });

    try {
      // Send HTTP POST request to backend
      console.log('üì§ Sending POST request to /api/summarize');
      console.log('üìä Request payload:', {
        textLength: transcription.length,
        summaryType: summaryType,
        endpoint: 'http://localhost:8000/api/summarize'
      });
      
      const response = await fetch('http://localhost:8000/api/summarize', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json', // Tell server we're sending JSON
        },
        body: JSON.stringify({
          text: transcription,        // The transcribed text to analyze
          summary_type: summaryType   // What kind of summary we want
        }),
      });

      console.log('üì® Response received:', {
        status: response.status,
        statusText: response.statusText,
        ok: response.ok,
        headers: Object.fromEntries(response.headers.entries())
      });

      // Check if request was successful
      if (!response.ok) {
        console.log('‚ùå Response not OK:', response.status);
        throw new Error(`HTTP error! status: ${response.status}`);
      }

      // Parse the JSON response from backend
      console.log('üìä Parsing JSON response');
      const summary: AISummary = await response.json();
      console.log('‚úÖ Summary received and parsed:', {
        hasSummary: !!summary.summary,
        hasKeyPoints: !!(summary.key_points && summary.key_points.length > 0),
        hasActionItems: !!(summary.action_items && summary.action_items.length > 0),
        hasError: !!summary.error,
        summaryPreview: summary.summary?.substring(0, 100) + '...'
      });
      
      console.log('üîÑ Setting AI summary state');
      setAiSummary(summary); // Update UI with summary

    } catch (error) {
      console.error('‚ùå Error generating summary:', error);
      console.log('üìä Summary generation error details:', {
        errorName: error instanceof Error ? error.name : 'Unknown',
        errorMessage: error instanceof Error ? error.message : String(error),
        errorStack: error instanceof Error ? error.stack : undefined
      });
      setError('Failed to generate AI summary. Please check if the backend is running.');
    } finally {
      // Always turn off loading state, whether success or failure
      console.log('üîÑ Setting isGeneratingSummary to false');
      setIsGeneratingSummary(false);
    }
  };

  /**
   * üßπ CLEAR FUNCTION
   * 
   * Resets the application to initial state
   * Useful for starting a new recording session
   */
  const clearAll = () => {
    console.log('üßπ clearAll called - resetting application state');
    console.log('üìä State before clearing:', {
      transcriptionLength: transcription.length,
      interimTextLength: interimText.length,
      hasAiSummary: !!aiSummary,
      hasError: !!error
    });
    
    setTranscription('');     // Clear transcribed text
    setInterimText('');       // Clear interim text
    setAiSummary(null);       // Clear AI summary
    setError('');             // Clear error messages
    
    console.log('‚úÖ All state cleared');
  };

  /**
   * üé® RENDER FUNCTION
   * 
   * This is the JSX that defines what the user sees
   * JSX is HTML-like syntax that React converts to DOM elements
   * 
   * STRUCTURE:
   * - Header with app title
   * - Main content area with:
   *   - Text display box (transcription + interim text)
   *   - Control buttons (Start/Stop, Clear)
   *   - Summary section (when available)
   *   - Error display (when needed)
   *   - Status indicator
   */
  
  console.log('üé® Rendering UI with current state:', {
    isRecording,
    connectionStatus,
    transcriptionLength: transcription.length,
    interimTextLength: interimText.length,
    hasError: !!error,
    hasAiSummary: !!aiSummary,
    isGeneratingSummary,
    showSummarySection: !!(aiSummary || isGeneratingSummary),
    clearButtonDisabled: !transcription && !aiSummary,
    recordButtonDisabled: connectionStatus === 'Connection Error'
  });

  return (
    <div className="simple-app">
      
      {/* üè† SIMPLE HEADER */}
      <div className="simple-header">
        <h1>üé§ Voice to Text</h1>
      </div>

      {/* üì± MAIN CONTENT CONTAINER */}
      <div className="simple-main">
        
        {/* üìù TEXT DISPLAY BOX */}
        {/* This shows the transcribed text and real-time interim text */}
        <div className="simple-textbox">
          
          {/* Final transcribed text - this is the "official" transcription */}
          {transcription && (
            <div className="transcription-text">{transcription}</div>
          )}
          
          {/* Interim text - shows what's being transcribed right now */}
          {/* This gives immediate feedback while speaking */}
          {interimText && (
            <div className="interim-text">{interimText}</div>
          )}
          
          {/* Placeholder text when nothing is transcribed yet */}
          {!transcription && !interimText && (
            <div className="placeholder-text">
              Click "Start" to begin recording...
            </div>
          )}
        </div>

        {/* üéõÔ∏è CONTROL BUTTONS */}
        <div className="simple-controls">
          
          {/* MAIN RECORD/STOP BUTTON */}
          {/* This button changes based on recording state */}
          <button
            className={`simple-btn ${isRecording ? 'stop-btn' : 'start-btn'}`}
            onClick={() => {
              console.log('üñ±Ô∏è Record/Stop button clicked:', {
                currentState: isRecording ? 'recording' : 'stopped',
                action: isRecording ? 'stopRecording' : 'startRecording'
              });
              return isRecording ? stopRecording() : startRecording();
            }}
            disabled={connectionStatus === 'Connection Error'} // Disable if connection failed
          >
            {isRecording ? (
              // STOP STATE: Show stop icon and "Stop & Summarize" text
              <>
                <MicOff size={20} />
                Stop & Summarize
              </>
            ) : (
              // START STATE: Show microphone icon and "Start Recording" text
              <>
                <Mic size={20} />
                Start Recording
              </>
            )}
          </button>
          
          {/* CLEAR BUTTON */}
          {/* Only enabled when there's content to clear */}
          <button
            className="simple-btn clear-btn"
            onClick={() => {
              console.log('üñ±Ô∏è Clear button clicked');
              clearAll();
            }}
            disabled={!transcription && !aiSummary} // Disable if nothing to clear
          >
            Clear
          </button>
        </div>

        {/* ü§ñ AI SUMMARY SECTION */}
        {/* Only show this section when generating summary or have results */}
        {(aiSummary || isGeneratingSummary) && (
          <div className="simple-summary">
            <h3>üìù Summary</h3>
            
            {/* LOADING STATE */}
            {isGeneratingSummary && (
              <div className="loading-text">Generating summary...</div>
            )}
            
            {/* SUMMARY RESULTS */}
            {aiSummary && !isGeneratingSummary && (
              <div className="summary-content">
                
                {/* ERROR HANDLING */}
                {aiSummary.error ? (
                  <div className="error-text">‚ùå {aiSummary.error}</div>
                ) : (
                  <>
                    {/* MAIN SUMMARY */}
                    {aiSummary.summary && (
                      <div className="summary-section">
                        <strong>Summary:</strong>
                        <p>{aiSummary.summary}</p>
                      </div>
                    )}
                    
                    {/* KEY POINTS */}
                    {aiSummary.key_points && aiSummary.key_points.length > 0 && (
                      <div className="summary-section">
                        <strong>Key Points:</strong>
                        <ul>
                          {aiSummary.key_points.map((point, index) => (
                            <li key={index}>{point}</li>
                          ))}
                        </ul>
                      </div>
                    )}
                    
                    {/* ACTION ITEMS */}
                    {aiSummary.action_items && aiSummary.action_items.length > 0 && (
                      <div className="summary-section">
                        <strong>Action Items:</strong>
                        <ul>
                          {aiSummary.action_items.map((item, index) => (
                            <li key={index}>
                              {item.task}
                              {item.responsible_party && ` - ${item.responsible_party}`}
                              {item.deadline && ` (Due: ${item.deadline})`}
                            </li>
                          ))}
                        </ul>
                      </div>
                    )}
                  </>
                )}
              </div>
            )}
          </div>
        )}

        {/* ‚ö†Ô∏è ERROR DISPLAY */}
        {/* Shows error messages when things go wrong */}
        {error && (
          <div className="simple-error">
            ‚ö†Ô∏è {error}
          </div>
        )}
        
        {/* üìä CONNECTION STATUS */}
        {/* Shows current connection status for debugging */}
        <div className="simple-status">
          Status: {connectionStatus}
        </div>
      </div>
    </div>
  );
}

// Export the App component so it can be imported in other files
export default App; 