// ğŸ“¦ IMPORTS - Bringing in code from other files and libraries

// React core - the main library for building user interfaces
// useState: Hook for managing component state (data that can change)
// useEffect: Hook for side effects (cleanup, API calls, etc.)
// useRef: Hook for accessing DOM elements or storing mutable values
import React, { useState, useEffect, useRef } from 'react';

// Lucide React - beautiful icons for our buttons
// Mic: Microphone icon for recording state
// MicOff: Microphone off icon for stopped state
import { Mic, MicOff } from 'lucide-react';

// Our custom CSS styles for making the app look good
import './App.css';

// TypeScript type definitions - these help prevent bugs by defining data shapes
// These interfaces define the structure of data we expect to receive/send
import { TranscriptionMessage, AISummary, ConnectionStatus, SummaryType } from './types';

/**
 * ğŸ¤ MAIN APP COMPONENT
 * 
 * This is the main React component that handles:
 * - Recording audio from the user's microphone
 * - Sending audio data to the backend via WebSocket
 * - Receiving real-time transcription from Deepgram
 * - Generating AI summaries using Google Gemini
 * - Managing all the UI state and user interactions
 * 
 * FLOW:
 * 1. User clicks "Start Recording"
 * 2. App requests microphone permission
 * 3. App connects to WebSocket backend
 * 4. Audio is streamed to backend â†’ Deepgram â†’ transcription appears
 * 5. User clicks "Stop & Summarize"
 * 6. App automatically generates AI summary
 */
function App() {
  console.log('ğŸš€ App component rendering/re-rendering');

  // ğŸ”„ STATE MANAGEMENT
  // React hooks for managing component state - these variables can change and trigger re-renders
  
  // ğŸ¤ RECORDING STATE: tracks whether we're currently recording audio
  // boolean = true/false value
  const [isRecording, setIsRecording] = useState<boolean>(false);
  
  // ğŸ“ TRANSCRIPTION TEXT: holds the final transcribed text from Deepgram
  // string = text value that gets displayed in the main text box
  const [transcription, setTranscription] = useState<string>('');
  
  // â±ï¸ INTERIM TEXT: temporary text being processed (live feedback)
  // This shows what's being transcribed in real-time before it's finalized
  const [interimText, setInterimText] = useState<string>('');
  
  // ğŸ”— CONNECTION STATUS: tracks connection to backend WebSocket
  // ConnectionStatus is a custom type defined in types.ts
  const [connectionStatus, setConnectionStatus] = useState<ConnectionStatus>('Disconnected');
  
  // âš ï¸ ERROR MESSAGES: for showing errors to user when things go wrong
  const [error, setError] = useState<string>('');
  
  // ğŸ¤– AI SUMMARY: holds the generated summary from Google Gemini
  // AISummary | null means it can be a summary object OR null (empty)
  const [aiSummary, setAiSummary] = useState<AISummary | null>(null);
  
  // â³ LOADING STATE: shows spinner/loading text when generating summary
  const [isGeneratingSummary, setIsGeneratingSummary] = useState<boolean>(false);

  // ğŸ“Š LOG CURRENT STATE VALUES - Debug information about our component state
  // This helps us track how our useState variables change over time
  console.log('ğŸ“Š Current State Values:', {
    
    // ğŸ¤ RECORDING STATE
    isRecording,                           // Boolean: Are we currently recording audio? (true/false)
    
    // ğŸ“ TEXT STATE  
    transcriptionLength: transcription.length,    // Number: How many characters in final transcription
    interimTextLength: interimText.length,        // Number: How many characters in live preview text
    
    // ğŸ”— CONNECTION STATE
    connectionStatus,                      // String: Current WebSocket connection status ("Connected", "Disconnected", etc.)
    
    // âš ï¸ ERROR STATE
    hasError: !!error,                     // Boolean: Do we have any error message? (!!) converts to true/false
    errorMessage: error,                   // String: The actual error message (empty string if no error)
    
    // ğŸ¤– AI SUMMARY STATE
    hasAiSummary: !!aiSummary,            // Boolean: Do we have an AI summary generated?
    isGeneratingSummary                    // Boolean: Are we currently generating a summary?
  });

  // ğŸ”— REFS FOR PERSISTENT OBJECTS
  // useRef creates references to objects that persist across re-renders
  // These don't trigger re-renders when changed (unlike useState)
  // 
  // WHY USE useRef INSTEAD OF useState?
  // - useState: For data that affects what the user sees (UI state)
  // - useRef: For "utility objects" that work behind the scenes
  // 
  // Think of useRef like a "storage box" that:
  // âœ… Keeps the same object between component re-renders
  // âœ… Doesn't cause re-renders when the object changes
  // âœ… Perfect for APIs, connections, and DOM references
  
  // ğŸŒ WEBSOCKET CONNECTION REF
  // WebSocket is a built-in browser API (no import needed) for real-time communication
  // 
  // WHAT IT DOES:
  // - Creates a persistent connection to the Python backend server
  // - Allows sending audio data TO the server instantly
  // - Allows receiving transcription data FROM the server instantly
  // - Like a phone line that stays open during the entire recording session
  // 
  // LIFECYCLE:
  // 1. null (initially - no connection)
  // 2. WebSocket object (when connected to ws://localhost:8000/ws)
  // 3. null again (when connection is closed)
  const websocketRef = useRef<WebSocket | null>(null);
  
  // ğŸ™ï¸ MEDIARECORDER REF
  // MediaRecorder is a built-in browser API (no import needed) for recording audio/video
  // 
  // WHAT IT DOES:
  // - Takes audio from your microphone (MediaStream)
  // - Converts it into compressed audio data (WebM format with Opus codec)
  // - Splits recording into small chunks (every 100ms)
  // - Triggers events when each chunk is ready to send
  // 
  // SIMPLE ANALOGY:
  // - Your microphone = Raw sound waves
  // - MediaRecorder = Digital tape recorder that processes and packages the sound
  // - Audio chunks = Small pieces of recorded audio sent continuously
  // 
  // LIFECYCLE:
  // 1. null (initially - no recorder)
  // 2. MediaRecorder object (when recording starts)
  // 3. null again (when recording stops and cleanup happens)
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  
  // ğŸ”Š AUDIO STREAM REF
  // MediaStream is a built-in browser API (no import needed) representing live audio/video
  // 
  // WHAT IT IS:
  // - The actual "live feed" from your microphone
  // - Contains audio tracks (channels of sound data)
  // - Like a "pipe" that continuously flows with audio data
  // - This is what MediaRecorder reads from to create recordings
  // 
  // SIMPLE ANALOGY:
  // - Think of water flowing through a pipe
  // - MediaStream = The pipe with audio "flowing" through it
  // - Your microphone = The source of the "audio water"
  // - MediaRecorder = A device that "collects" audio from this pipe
  // 
  // HOW WE GET IT:
  // navigator.mediaDevices.getUserMedia({ audio: true })
  // â†‘ This asks the browser: "Give me access to the user's microphone"
  // â†‘ Returns a MediaStream object with live audio data
  // 
  // WHY STORE IN REF:
  // - We need to stop/cleanup the stream when recording ends
  // - Stopping releases the microphone (turns off the red recording indicator)
  // - If we don't cleanup, the microphone stays "on" even after stopping
  // 
  // LIFECYCLE:
  // 1. null (initially - no microphone access)
  // 2. MediaStream object (when getUserMedia() succeeds)
  // 3. null again (when we stop all tracks and cleanup)
  const audioStreamRef = useRef<MediaStream | null>(null);

  // ğŸ” LOG REF STATUS - Debug information about our persistent objects
  // This console.log helps us understand what's happening with our refs during development
  console.log('ğŸ”— Refs Status:', {
    
    // ğŸŒ WEBSOCKET REFERENCE DETAILS
    // These help us understand the WebSocket connection state
    
    websocketRef: websocketRef,                    // The useRef container itself (always exists)
    websocketRefCurrent: websocketRef.current,     // The actual WebSocket object inside the ref (null when disconnected)
    hasWebSocket: !!websocketRef.current,          // Boolean: Do we have a WebSocket? (!!) converts to true/false
    
    // WebSocket Connection States (0-3 numbers that tell us connection status)
    websocketReadyState: websocketRef.current?.readyState,  // Raw number: 0=CONNECTING, 1=OPEN, 2=CLOSING, 3=CLOSED
    websocketReadyStateText: websocketRef.current ?         // Human-readable version of the state
      ['CONNECTING', 'OPEN', 'CLOSING', 'CLOSED'][websocketRef.current.readyState] : 'null',
    
    // ğŸ“Š ADDITIONAL WEBSOCKET PROPERTIES
    // These give us more detailed info about the WebSocket connection
    
    websocketUrl: websocketRef.current?.url,               // The URL we connected to (e.g., "ws://localhost:8000/ws")
    websocketProtocol: websocketRef.current?.protocol,     // Sub-protocol used (usually empty string)
    websocketExtensions: websocketRef.current?.extensions, // WebSocket extensions (usually empty string)
    websocketBufferedAmount: websocketRef.current?.bufferedAmount, // Bytes waiting to be sent (should be 0 usually)
    
    // ğŸ™ï¸ MEDIARECORDER REFERENCE DETAILS
    // These help us understand the audio recording state
    
    hasMediaRecorder: !!mediaRecorderRef.current,          // Boolean: Do we have a MediaRecorder object?
    mediaRecorderState: mediaRecorderRef.current?.state,   // Recording state: "inactive", "recording", "paused"
    
    // ğŸ”Š AUDIO STREAM REFERENCE DETAILS  
    // These help us understand the microphone stream state
    
    hasAudioStream: !!audioStreamRef.current,              // Boolean: Do we have access to microphone?
    audioStreamActive: audioStreamRef.current?.active,     // Boolean: Is the microphone stream still active?
    audioTrackCount: audioStreamRef.current?.getTracks().length || 0  // Number of audio tracks (usually 1 for mono)
  });

  // ğŸ” SEPARATE DETAILED WEBSOCKET LOG - Deep dive into WebSocket object properties
  // This gives us a complete picture of the WebSocket instance when it exists
  if (websocketRef.current) {
    console.log('ğŸŒ WebSocket Instance Details:', {
      
      // ğŸ—ï¸ OBJECT INFORMATION
      websocketObject: websocketRef.current,        // The actual WebSocket object (you can expand this in dev tools)
      constructor: websocketRef.current.constructor.name,  // Always "WebSocket" - confirms object type
      
      // ğŸ“Š CONNECTION STATUS
      readyState: websocketRef.current.readyState,          // Number 0-3: Current connection state
      // 0 = CONNECTING (trying to connect)
      // 1 = OPEN (connected and ready)  
      // 2 = CLOSING (connection is closing)
      // 3 = CLOSED (connection is closed)
      
      // ğŸ”— CONNECTION DETAILS
      url: websocketRef.current.url,                        // Full WebSocket URL (e.g., "ws://localhost:8000/ws")
      protocol: websocketRef.current.protocol,              // Sub-protocol negotiated during handshake (usually empty)
      extensions: websocketRef.current.extensions,          // WebSocket extensions selected by server (usually empty)
      
      // ğŸ“¤ SEND BUFFER STATUS
      bufferedAmount: websocketRef.current.bufferedAmount,  // Bytes waiting to be sent (should be 0 for real-time)
      
      // ğŸ“ DATA FORMAT
      binaryType: websocketRef.current.binaryType,          // How binary data is received: "blob" or "arraybuffer"
      
      // ğŸ¯ EVENT HANDLERS (these show as "function" or "null")
      // These are the callback functions we set up to handle WebSocket events
      onopen: typeof websocketRef.current.onopen,           // Function called when connection opens
      onmessage: typeof websocketRef.current.onmessage,     // Function called when message arrives
      onclose: typeof websocketRef.current.onclose,         // Function called when connection closes  
      onerror: typeof websocketRef.current.onerror          // Function called when error occurs
    });
  } else {
    console.log('ğŸŒ WebSocket Instance: null (not connected)');
    console.log('ğŸ’¡ This means we either haven\'t connected yet, or the connection was closed/failed');
  }

  // ğŸ§¹ CLEANUP EFFECT
  // useEffect with empty dependency array [] runs once when component mounts
  // The return function runs when component unmounts (cleanup)
  useEffect(() => {
    console.log('ğŸ—ï¸ App component mounted - setting up cleanup effect');
    
    return () => {
      console.log('ğŸ§¹ App component unmounting - running cleanup');
      stopRecording();
      if (websocketRef.current) {
        console.log('ğŸ”Œ Closing WebSocket during cleanup');
        websocketRef.current.close();
      }
    };
  }, []); // Empty array means this effect runs only once

  // ğŸ” STATE CHANGE EFFECT - Log when important state changes
  useEffect(() => {
    console.log('ğŸ”„ State Change Detected:', {
      isRecording,
      connectionStatus,
      transcriptionLength: transcription.length,
      hasError: !!error
    });
  }, [isRecording, connectionStatus, transcription, error]);

  /**
   * ğŸŒ WEBSOCKET CONNECTION FUNCTION
   * 
   * Establishes real-time connection to Python backend
   * WebSocket allows bidirectional communication (unlike HTTP requests)
   * 
   * PROCESS:
   * 1. Create WebSocket connection to ws://localhost:8000/ws
   * 2. Set up event handlers for open, message, close, error
   * 3. Handle incoming transcription messages from Deepgram
   * 4. Update UI state based on messages received
   * 
   * @returns Promise that resolves when connection is established
   */
  const connectWebSocket = (): Promise<void> => {
    // We'll store a reference to the Promise so we can inspect it later within the function
    let promiseRef: Promise<void>;
    
    console.log('ğŸŒ ==================== WEBSOCKET CONNECTION START ====================');
    console.log('ğŸŒ connectWebSocket called');
    // Log immediately after promiseRef is assigned to view its initial state (<pending>)
    // Note: The console will update this object's state live in devtools.
    setTimeout(() => {
      console.log('ğŸ“Œ [connectWebSocket] Promise state right after creation:', promiseRef);
    }, 0);
    console.log('ğŸ“Š WebSocket Info: What is WebSocket?');
    console.log('   â€¢ WebSocket = Persistent, bidirectional connection (like a phone call)');
    console.log('   â€¢ HTTP = One-time request/response (like sending a letter)');
    console.log('   â€¢ WebSocket stays open for real-time communication');
    console.log('   â€¢ Perfect for live audio streaming and instant transcription');
    
    promiseRef = new Promise((resolve, reject) => {
      try {
        console.log('ğŸ”„ Setting connection status to Connecting');
        console.log('ğŸ“Š UI State Update: connectionStatus = "Connecting"');
        // Update UI to show we're trying to connect
        setConnectionStatus('Connecting');
        
        console.log('ğŸ”— Creating WebSocket connection to ws://localhost:8000/ws');
        console.log('ğŸ“Š WebSocket URL Breakdown:');
        console.log('   â€¢ Protocol: ws:// (WebSocket, not http://)');
        console.log('   â€¢ Host: localhost (same computer)');
        console.log('   â€¢ Port: 8000 (where Python backend is running)');
        console.log('   â€¢ Path: /ws (WebSocket endpoint on backend)');
        
        // Create new WebSocket connection to backend
        const ws = new WebSocket('ws://localhost:8000/ws');
        console.log('âœ… WebSocket object created');
        console.log('ğŸ“Š Initial WebSocket state:', {
          readyState: ws.readyState,
          readyStateText: ['CONNECTING', 'OPEN', 'CLOSING', 'CLOSED'][ws.readyState],
          url: ws.url,
          protocol: ws.protocol,
          extensions: ws.extensions
        });
        
        websocketRef.current = ws;
        console.log('ğŸ“¦ WebSocket stored in ref for later use');

        // ğŸ‰ CONNECTION OPENED - Backend is ready to receive audio
        ws.onopen = () => {
          console.log('ğŸ‰ ==================== WEBSOCKET OPENED ====================');
          console.log('âœ… WebSocket connected successfully!');
          console.log('ğŸ“Š Connection Details:');
          console.log('   â€¢ Client (this app) â†”ï¸ Server (Python backend) connection established');
          console.log('   â€¢ Can now send audio data TO server');
          console.log('   â€¢ Can now receive transcription FROM server');
          console.log('   â€¢ Connection is persistent (stays open until closed)');
          console.log('ğŸ“Š WebSocket State After Opening:', {
            readyState: ws.readyState,
            readyStateText: ['CONNECTING', 'OPEN', 'CLOSING', 'CLOSED'][ws.readyState],
            bufferedAmount: ws.bufferedAmount
          });
          
          console.log('ğŸ”„ Updating UI: connectionStatus = "Connected"');
          setConnectionStatus('Connected');
          setError(''); // Clear any previous errors
          console.log('ğŸ”„ Clearing previous errors; about to resolve promise...');
          console.log('ğŸ“Œ [connectWebSocket] Promise state is currently PENDING. Calling resolve() to fulfill it now.');
          // Log the promise just before fulfilling
          console.log('ğŸ“Œ [connectWebSocket] Promise about to be resolved. Current state:', promiseRef);
          resolve(); // Promise succeeds - connection established
          // Log right after resolve to confirm state change
          setTimeout(() => {
            console.log('ğŸ“Œ [connectWebSocket] Promise after resolve call (should be <fulfilled>):', promiseRef);
          }, 0);
          console.log('âœ… resolve() called. Promise is now FULFILLED and awaiting callers will resume.');
        };

        // ğŸ“¨ MESSAGE RECEIVED - Backend sent us transcription data
        ws.onmessage = (event) => {
          console.log('ğŸ“¨ ==================== WEBSOCKET MESSAGE RECEIVED ====================');
          console.log('ğŸ“¨ WebSocket message received from server!');
          console.log('ğŸ“Š Raw Message Details:', {
            dataType: typeof event.data,
            dataSize: event.data.length,
            timestamp: new Date().toISOString(),
            rawData: event.data
          });
          console.log('ğŸ“Š Message Flow: Server â†’ Client (this app)');
          console.log('   â€¢ Server processed audio and generated transcription');
          console.log('   â€¢ Server sent result back through WebSocket');
          console.log('   â€¢ This event handler receives and processes the message');
          
          try {
            console.log('ğŸ”„ Parsing JSON message from backend...');
            // Parse JSON message from backend
            const data: TranscriptionMessage = JSON.parse(event.data);
            console.log('âœ… JSON parsing successful!');
            console.log('ğŸ“Š Parsed message data:', data);
            console.log('ğŸ“Š Message Structure Analysis:', {
              messageType: data.type,
              hasText: !!data.text,
              isFinal: data.is_final,
              hasFullTranscript: !!data.full_transcript,
              hasMessage: !!data.message
            });
            
            // Handle different types of messages
            console.log('ğŸ”„ Processing message based on type...');
            switch (data.type) {
              case 'transcription':
                console.log('ğŸ“ ==================== TRANSCRIPTION MESSAGE ====================');
                console.log('ğŸ“ Processing transcription message from Deepgram!');
                console.log('ğŸ“Š Transcription Details:', {
                  text: data.text,
                  is_final: data.is_final,
                  full_transcript_length: data.full_transcript?.length || 0,
                  textPreview: data.text ? data.text.substring(0, 50) + '...' : 'No text'
                });
                console.log('ğŸ“Š Transcription Flow:');
                console.log('   â€¢ Your voice â†’ Microphone â†’ MediaRecorder â†’ WebSocket â†’ Server');
                console.log('   â€¢ Server â†’ Deepgram AI â†’ Transcription â†’ WebSocket â†’ This app');
                
                // This is transcribed text from Deepgram
                if (data.text) {
                  if (data.is_final) {
                    // Final text - update main transcription
                    console.log('âœ… ==================== FINAL TRANSCRIPTION ====================');
                    console.log('âœ… Final transcription received - this is the confirmed text!');
                    console.log('ğŸ“Š Final Text Details:', {
                      finalText: data.text,
                      fullTranscriptLength: data.full_transcript?.length || 0,
                      fullTranscriptPreview: data.full_transcript ? data.full_transcript.substring(0, 100) + '...' : 'No full transcript'
                    });
                    console.log('ğŸ”„ Updating main transcription state...');
                    setTranscription(data.full_transcript || '');
                    setInterimText(''); // Clear interim text
                    console.log('ğŸ”„ Cleared interim text (no longer needed)');
                    console.log('âœ… UI updated with final transcription!');
                  } else {
                    // Interim text - show what's being processed
                    console.log('â±ï¸ ==================== INTERIM TRANSCRIPTION ====================');
                    console.log('â±ï¸ Interim transcription - live preview while speaking!');
                    console.log('ğŸ“Š Interim Details:', {
                      interimText: data.text,
                      textLength: data.text.length,
                      isTemporary: true
                    });
                    console.log('ğŸ“Š Interim vs Final:');
                    console.log('   â€¢ Interim = Live preview (may change as you continue speaking)');
                    console.log('   â€¢ Final = Confirmed text (won\'t change anymore)');
                    console.log('ğŸ”„ Updating interim text state...');
                    setInterimText(data.text);
                    console.log('âœ… UI updated with interim transcription!');
                  }
                } else {
                  console.log('âš ï¸ Transcription message received but no text content');
                }
                break;
              
              case 'connection_status':
                console.log('ğŸ”— ==================== CONNECTION STATUS MESSAGE ====================');
                console.log('ğŸ”— Connection status update from server:', data.message);
                console.log('ğŸ“Š Status Details:', {
                  newStatus: data.message,
                  timestamp: new Date().toISOString()
                });
                console.log('ğŸ“Š Status Flow: Server monitoring â†’ Status change â†’ WebSocket â†’ UI update');
                // Backend is telling us about connection status
                if (data.message) {
                  console.log('ğŸ”„ Updating connection status in UI...');
                  setConnectionStatus(data.message as ConnectionStatus);
                  console.log('âœ… Connection status updated!');
                }
                break;
              
              case 'error':
                console.log('âŒ ==================== ERROR MESSAGE ====================');
                console.log('âŒ Error message received from backend:', data.message);
                console.log('ğŸ“Š Error Details:', {
                  errorMessage: data.message,
                  timestamp: new Date().toISOString(),
                  source: 'WebSocket Server'
                });
                console.log('ğŸ“Š Error Flow: Server error â†’ WebSocket â†’ Client error handling â†’ UI error display');
                // Something went wrong on the backend
                console.log('ğŸ”„ Setting error state for UI display...');
                setError(data.message || 'Unknown error occurred');
                console.log('âœ… Error state updated - user will see error message');
                break;
                
              default:
                console.log('âš ï¸ ==================== UNKNOWN MESSAGE TYPE ====================');
                console.log('âš ï¸ Received message with unknown type:', data.type);
                console.log('ğŸ“Š Unknown Message Details:', data);
                break;
            }
            console.log('âœ… Message processing complete!');
          } catch (error) {
            console.error('âŒ ==================== MESSAGE PARSING ERROR ====================');
            console.error('âŒ Error parsing WebSocket message:', error);
            console.log('ğŸ“Š Parsing Error Details:', {
              errorType: error instanceof Error ? error.name : 'Unknown',
              errorMessage: error instanceof Error ? error.message : String(error),
              rawMessageData: event.data,
              dataType: typeof event.data,
              dataLength: event.data.length
            });
            console.log('ğŸ“„ Raw message data that failed to parse:', event.data);
            console.log('ğŸ’¡ Possible causes:');
            console.log('   â€¢ Server sent invalid JSON');
            console.log('   â€¢ Message format changed');
            console.log('   â€¢ Network corruption');
          }
        };

        // ğŸ”Œ CONNECTION CLOSED - Backend disconnected
        ws.onclose = (event) => {
          console.log('ğŸ”Œ ==================== WEBSOCKET CLOSED ====================');
          console.log('ğŸ”Œ WebSocket connection closed!');
          console.log('ğŸ“Š Close Event Details:', {
            code: event.code,
            reason: event.reason,
            wasClean: event.wasClean,
            timestamp: new Date().toISOString()
          });
          console.log('ğŸ“Š Close Code Meaning:');
          const closeReasons: { [key: number]: string } = {
            1000: 'Normal closure',
            1001: 'Going away',
            1002: 'Protocol error',
            1003: 'Unsupported data',
            1006: 'Abnormal closure',
            1011: 'Server error',
            1015: 'TLS handshake failure'
          };
          console.log(`   â€¢ Code ${event.code}: ${closeReasons[event.code] || 'Unknown reason'}`);
          console.log('ğŸ“Š Connection Lifecycle: CONNECTING â†’ OPEN â†’ CLOSING â†’ CLOSED âœ…');
          console.log('ğŸ”„ Updating UI connection status to Disconnected...');
          setConnectionStatus('Disconnected');
          console.log('âœ… UI updated - user will see disconnected status');
        };

        // âŒ CONNECTION ERROR - Something went wrong
        ws.onerror = (error) => {
          console.error('âŒ ==================== WEBSOCKET ERROR ====================');
          console.error('âŒ WebSocket connection error occurred!');
          console.error('ğŸ“Š Error Event:', error);
          console.log('ğŸ“Š Common WebSocket Error Causes:');
          console.log('   â€¢ Backend server not running (most common)');
          console.log('   â€¢ Wrong URL or port number');
          console.log('   â€¢ Network connectivity issues');
          console.log('   â€¢ Firewall blocking connection');
          console.log('   â€¢ Server overloaded or crashed');
          console.log('ğŸ”„ Setting connection status to error state...');
          setConnectionStatus('Connection Error');
          console.log('ğŸ”„ Setting user-friendly error message...');
          setError('Connection failed. Make sure the backend is running.');
          console.log('ğŸ”„ Rejecting promise due to WebSocket error');
          console.log('âŒ Connection attempt failed - promise will be rejected');
          reject(error); // Promise fails - connection failed
        };

      } catch (error) {
        console.error('âŒ Failed to create WebSocket connection:', error);
        setConnectionStatus('Connection Error');
        setError('Failed to connect to backend');
        reject(error);
      }
    });
    return promiseRef;
  };

  /**
   * ğŸ™ï¸ START RECORDING FUNCTION
   * 
   * Initiates the voice recording process
   * 
   * PROCESS:
   * 1. Connect to WebSocket backend
   * 2. Request microphone permission from browser
   * 3. Create MediaRecorder to capture audio
   * 4. Set up audio streaming to backend
   * 5. Start recording with optimal settings
   * 
   * AUDIO SETTINGS:
   * - sampleRate: 16000 Hz (CD quality, good for speech recognition)
   * - channelCount: 1 (mono - single channel is sufficient for speech)
   * - echoCancellation: true (removes echo feedback)
   * - noiseSuppression: true (reduces background noise)
   */
  const startRecording = async () => {
    console.log('ğŸ™ï¸ startRecording called');
    console.log('ğŸ“Š Pre-recording state check - What we have before starting:', {
      
      // ğŸ¤ CURRENT RECORDING STATUS
      isRecording,                           // Boolean: Should be false (we're about to start)
      
      // ğŸ”— CONNECTION STATUS  
      connectionStatus,                      // String: Current connection state
      hasWebSocket: !!websocketRef.current, // Boolean: Do we already have a WebSocket connection?
      
      // ğŸ¬ RECORDING EQUIPMENT STATUS
      hasMediaRecorder: !!mediaRecorderRef.current,  // Boolean: Do we have a MediaRecorder from previous session?
      hasAudioStream: !!audioStreamRef.current       // Boolean: Do we have microphone access from previous session?
      
      // ğŸ’¡ WHAT WE EXPECT:
      // - isRecording should be false (we're starting)
      // - connectionStatus should be "Disconnected" (we haven't connected yet)
      // - hasWebSocket should be false (no connection yet)
      // - hasMediaRecorder should be false (no recorder yet)
      // - hasAudioStream should be false (no microphone access yet)
    });

    try {
      console.log('ğŸ”„ Clearing previous errors');
      setError(''); // Clear any previous errors
      
      // STEP 1: Connect to WebSocket backend first
      console.log('ğŸŒ Step 1: Connecting to WebSocket backend');
      await connectWebSocket();
      console.log('âœ… WebSocket connection established');
      
      // STEP 2: Request microphone access from browser
      // This will show a permission dialog to the user
      console.log('ğŸ¤ Step 2: Requesting microphone access with settings:', {
        sampleRate: 16000,
        channelCount: 1,
        echoCancellation: true,
        noiseSuppression: true
      });
      
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          sampleRate: 16000,        // 16kHz sample rate (good for speech)
          channelCount: 1,          // Mono audio (single channel)
          echoCancellation: true,   // Remove echo
          noiseSuppression: true    // Reduce background noise
        } 
      });
      
      console.log('âœ… Microphone access granted - Details about the audio stream:', {
        
        // ğŸ†” STREAM IDENTIFICATION
        streamId: stream.id,                   // Unique ID for this MediaStream instance
        streamActive: stream.active,           // Boolean: Is the stream currently active?
        
        // ğŸ“Š STREAM COMPOSITION
        trackCount: stream.getTracks().length, // Total number of tracks (audio + video, usually just 1 audio)
        
        // ğŸµ AUDIO TRACK DETAILS
        // Each audio track represents a channel of audio data from a source (microphone)
        audioTracks: stream.getAudioTracks().map(track => ({
          id: track.id,                        // Unique ID for this audio track
          label: track.label,                  // Human-readable name (e.g., "Built-in Microphone")
          enabled: track.enabled,              // Boolean: Is this track enabled? (should be true)
          readyState: track.readyState         // String: "live" = working, "ended" = stopped
        }))
        
        // ğŸ’¡ WHAT THIS MEANS:
        // - We now have a "pipe" of live audio data from the user's microphone
        // - This stream will be fed into MediaRecorder to create audio chunks
        // - Each track represents one audio channel (mono = 1 track, stereo = 2 tracks)
      });
      
      // Store the audio stream for later cleanup
      audioStreamRef.current = stream;
      
      // STEP 3: Create MediaRecorder to capture and encode audio
      // WebM with Opus codec provides good compression for real-time streaming
      console.log('ğŸ¬ Step 3: Creating MediaRecorder with mimeType: audio/webm;codecs=opus');
      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: 'audio/webm;codecs=opus'
      });
      
      console.log('ğŸ“Š MediaRecorder created - Details about the audio recorder:', {
        
        // ğŸ“Š RECORDER STATUS
        state: mediaRecorder.state,                    // String: Current recorder state ("inactive", "recording", "paused")
        
        // ğŸµ AUDIO FORMAT SETTINGS
        mimeType: mediaRecorder.mimeType,              // String: Audio format (e.g., "audio/webm;codecs=opus")
        audioBitsPerSecond: mediaRecorder.audioBitsPerSecond, // Number: Audio quality in bits per second
        
        // ğŸ’¡ WHAT THIS MEANS:
        // - MediaRecorder is ready to convert live audio into compressed audio chunks
        // - WebM with Opus codec provides good compression for real-time streaming
        // - State "inactive" means it's created but not recording yet
        // - We'll call start() next to begin recording
      });
      
      mediaRecorderRef.current = mediaRecorder;
      
              // STEP 4: Set up audio data handler
        // This function runs every time MediaRecorder has audio data ready
        console.log('ğŸ“¡ Step 4: Setting up audio data handler');
        mediaRecorder.ondataavailable = (event) => {
          console.log('ğŸ“¡ ==================== AUDIO DATA AVAILABLE ====================');
          console.log('ğŸ“¡ MediaRecorder has audio data ready to send!');
          console.log('ğŸ“Š Audio Data Details:', {
            dataSize: event.data.size,
            dataType: event.data.type,
            dataSizeKB: (event.data.size / 1024).toFixed(2) + ' KB',
            timestamp: new Date().toISOString(),
            websocketState: websocketRef.current?.readyState,
            websocketStateText: websocketRef.current ? 
              ['CONNECTING', 'OPEN', 'CLOSING', 'CLOSED'][websocketRef.current.readyState] : 'null',
            websocketOpen: websocketRef.current?.readyState === WebSocket.OPEN
          });
          console.log('ğŸ“Š Audio Data Flow:');
          console.log('   â€¢ Microphone captures sound waves');
          console.log('   â€¢ MediaRecorder converts to digital audio (WebM/Opus)');
          console.log('   â€¢ Audio split into 100ms chunks for real-time streaming');
          console.log('   â€¢ Each chunk triggers this ondataavailable event');
          console.log('   â€¢ We send chunk through WebSocket to server for transcription');
          
          // Only send if we have data and WebSocket is connected
          if (event.data.size > 0 && websocketRef.current?.readyState === WebSocket.OPEN) {
            console.log('âœ… Conditions met for sending audio data:');
            console.log('   âœ… Data size > 0 (' + event.data.size + ' bytes)');
            console.log('   âœ… WebSocket is OPEN and ready');
            console.log('ğŸ“¤ ==================== SENDING AUDIO TO SERVER ====================');
            console.log('ğŸ“¤ Sending audio chunk to backend via WebSocket...');
            
            // Send raw audio data to backend for transcription
            websocketRef.current.send(event.data);
            
            console.log('âœ… Audio data sent successfully!');
            console.log('ğŸ“Š What happens next:');
            console.log('   â€¢ Server receives audio chunk');
            console.log('   â€¢ Server forwards to Deepgram for transcription');
            console.log('   â€¢ Deepgram processes audio and returns text');
            console.log('   â€¢ Server sends transcription back via WebSocket');
            console.log('   â€¢ We receive transcription in onmessage handler');
          } else {
            console.log('âŒ ==================== NOT SENDING AUDIO ====================');
            console.log('âŒ Cannot send audio data - conditions not met:');
            if (event.data.size === 0) {
              console.log('   âŒ No audio data available (size = 0)');
              console.log('   ğŸ’¡ This can happen if microphone is muted or no sound detected');
            } else {
              console.log('   âœ… Audio data available (' + event.data.size + ' bytes)');
            }
            
            if (websocketRef.current?.readyState !== WebSocket.OPEN) {
              console.log('   âŒ WebSocket not open (state: ' + 
                (websocketRef.current ? 
                  ['CONNECTING', 'OPEN', 'CLOSING', 'CLOSED'][websocketRef.current.readyState] : 'null') + ')');
              console.log('   ğŸ’¡ WebSocket must be OPEN to send data');
            } else {
              console.log('   âœ… WebSocket is OPEN and ready');
            }
            
            console.log('âš ï¸ Audio chunk will be discarded (not sent to server)');
          }
        };
      
      // STEP 5: Start recording
      // 100ms intervals = send audio data every 100 milliseconds for real-time processing
      console.log('ğŸ¬ Step 5: Starting MediaRecorder with 100ms intervals');
      mediaRecorder.start(100);
      
      console.log('ğŸ”„ Setting isRecording to true');
      setIsRecording(true); // Update UI state
      
      console.log('âœ… Recording started successfully');
      
    } catch (error) {
      console.error('âŒ Error starting recording:', error);
      console.log('ğŸ“Š Error details:', {
        errorName: error instanceof Error ? error.name : 'Unknown',
        errorMessage: error instanceof Error ? error.message : String(error),
        errorStack: error instanceof Error ? error.stack : undefined
      });
      setError('Failed to start recording. Please check microphone permissions.');
    }
  };

  /**
   * ğŸ›‘ STOP RECORDING FUNCTION
   * 
   * Stops the recording and automatically generates AI summary
   * 
   * PROCESS:
   * 1. Stop MediaRecorder
   * 2. Stop and cleanup audio stream
   * 3. Close WebSocket connection
   * 4. Wait briefly for final transcription
   * 5. Automatically generate AI summary
   * 
   * AUTO-SUMMARY FEATURE:
   * After stopping, the app automatically generates a summary
   * This provides immediate value without requiring extra clicks
   */
  const stopRecording = () => {
    console.log('ğŸ›‘ stopRecording called');
    
    try {
      // ğŸ” DEBUG: Log initial stopRecording state - What we have when stopping
      console.log('ğŸ›‘ stopRecording called - Current state before cleanup:', {
        
        // ğŸ¤ RECORDING STATUS
        isRecording,                                    // Boolean: Should be true (we're currently recording)
        
        // ğŸ¬ MEDIARECORDER STATUS
        mediaRecorderState: mediaRecorderRef.current?.state,  // String: Should be "recording" if active
        
        // ğŸ”Š AUDIO STREAM STATUS  
        hasAudioStream: !!audioStreamRef.current,      // Boolean: Do we have microphone access?
        
        // ğŸŒ WEBSOCKET STATUS
        hasWebSocket: !!websocketRef.current,          // Boolean: Do we have WebSocket connection?
        
        // ğŸ“ TRANSCRIPTION STATUS
        transcriptionLength: transcription.length,      // Number: How much text we've transcribed so far
        
        // ğŸ’¡ WHAT WE EXPECT:
        // - isRecording should be true (we're stopping an active recording)
        // - mediaRecorderState should be "recording" (actively recording)
        // - hasAudioStream should be true (we have microphone access)
        // - hasWebSocket should be true (we have active connection)
        // - transcriptionLength should be > 0 if we spoke during recording
      });
      
      // STEP 1: Stop the MediaRecorder if it's active
      if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
        console.log('ğŸ¬ Stopping MediaRecorder (current state:', mediaRecorderRef.current.state + ')');
        mediaRecorderRef.current.stop();
        console.log('âœ… MediaRecorder stopped');
      } else {
        console.log('âš ï¸ MediaRecorder not active or not available:', {
          hasMediaRecorder: !!mediaRecorderRef.current,
          state: mediaRecorderRef.current?.state
        });
      }
      
      // STEP 2: Stop the audio stream and release microphone
      if (audioStreamRef.current) {
        console.log('ğŸ”Š Stopping audio stream tracks');
        const tracks = audioStreamRef.current.getTracks();
        console.log('ğŸ“Š Audio tracks to stop:', tracks.length);
        
        // Stop all audio tracks (releases microphone access)
        tracks.forEach((track, index) => {
          console.log(`ğŸ›‘ Stopping audio track ${index} - Releasing microphone:`, {
            
            // ğŸ†” TRACK IDENTIFICATION
            id: track.id,                     // Unique ID for this audio track
            label: track.label,               // Human-readable name (e.g., "Built-in Microphone")
            
            // ğŸ“Š TRACK STATUS BEFORE STOPPING
            readyState: track.readyState      // String: Should be "live" before we stop it
            
            // ğŸ’¡ WHAT HAPPENS WHEN WE CALL track.stop():
            // - Microphone access is released (red recording indicator turns off)
            // - Track readyState changes from "live" to "ended"
            // - Audio stream stops flowing
            // - Browser knows we're done with the microphone
          });
          track.stop();
        });
        
        audioStreamRef.current = null;
        console.log('âœ… Audio stream tracks stopped and reference cleared');
      } else {
        console.log('âš ï¸ No audio stream to stop');
      }
      
      // STEP 3: Close WebSocket connection
      if (websocketRef.current) {
        console.log('ğŸ”Œ ==================== CLOSING WEBSOCKET ====================');
        console.log('ğŸ”Œ Closing WebSocket connection...');
        console.log('ğŸ“Š WebSocket State Before Closing:', {
          readyState: websocketRef.current.readyState,          // Numeric readyState (0-3) indicating connection status
          readyStateText: ['CONNECTING', 'OPEN', 'CLOSING', 'CLOSED'][websocketRef.current.readyState],
          url: websocketRef.current.url,
          bufferedAmount: websocketRef.current.bufferedAmount
        });
        console.log('ğŸ“Š Why Close WebSocket?');
        console.log('   â€¢ Recording stopped - no more audio to send');
        console.log('   â€¢ Prevents unnecessary network connection');
        console.log('   â€¢ Cleans up resources properly');
        console.log('   â€¢ Server can free up connection slot');
        
        console.log('ğŸ”„ Calling websocket.close()...');
        websocketRef.current.close();
        console.log('ğŸ”„ Clearing WebSocket reference...');
        websocketRef.current = null;
        console.log('âœ… WebSocket closed and reference cleared');
        console.log('ğŸ“Š Connection Lifecycle Complete: CONNECTING â†’ OPEN â†’ CLOSING â†’ CLOSED âœ…');
      } else {
        console.log('âš ï¸ ==================== NO WEBSOCKET TO CLOSE ====================');
        console.log('âš ï¸ No WebSocket connection to close');
        console.log('ğŸ’¡ This can happen if:');
        console.log('   â€¢ Connection was never established');
        console.log('   â€¢ Connection already closed due to error');
        console.log('   â€¢ Multiple stop recording calls');
      }
      
      // STEP 4: Update UI state
      console.log('ğŸ”„ Setting isRecording to false');
      setIsRecording(false);
      console.log('âœ… isRecording state updated to false');
      
      // STEP 5: ğŸ¤– AUTO-GENERATE SUMMARY
      // Wait 1 second for any final transcription to arrive, then generate summary
      console.log('â° Setting timeout for auto-summary generation (1 second)');
      setTimeout(() => {
        console.log('â° Timeout reached for auto-summary');
        console.log('ğŸ“Š Current transcription length:', transcription.length);
        console.log('ğŸ“„ Transcription preview:', transcription.substring(0, 100) + '...');
        
        if (transcription.trim()) { // Only if we have transcribed text
          console.log('ğŸ¤– Transcription available - generating summary automatically');
          generateSummary();
        } else {
          console.log('âš ï¸ No transcription available - skipping auto-summary');
        }
      }, 1000);
      
    } catch (error) {
      console.error('âŒ Error stopping recording:', error);
      console.log('ğŸ“Š Stop recording error details:', {
        errorName: error instanceof Error ? error.name : 'Unknown',
        errorMessage: error instanceof Error ? error.message : String(error)
      });
      setError('Error stopping recording');
    }
  };

  /**
   * ğŸ¤– GENERATE AI SUMMARY FUNCTION
   * 
   * Sends transcribed text to backend for AI analysis using Google Gemini
   * 
   * PROCESS:
   * 1. Validate we have text to summarize
   * 2. Send POST request to backend /api/summarize endpoint
   * 3. Backend forwards to Google Gemini AI
   * 4. Receive structured summary with key points, action items, etc.
   * 5. Display results in UI
   * 
   * SUMMARY TYPES:
   * - 'meeting': Comprehensive meeting summary (default)
   * - 'action_items': Focus on tasks and to-dos
   * - 'key_points': Main takeaways and important points
   * - 'speaker_analysis': Per-speaker breakdown (if diarization enabled)
   * 
   * @param summaryType - Type of summary to generate
   */
  const generateSummary = async (summaryType: SummaryType = 'meeting') => {
    console.log('ğŸ¤– generateSummary called with type:', summaryType);
    
    // Validate we have content to summarize
    if (!transcription.trim()) {
      console.log('âŒ No transcription available to summarize');
      setError('No transcription available to summarize');
      return;
    }

    console.log('ğŸ“Š Transcription validation passed:', {
      transcriptionLength: transcription.length,
      transcriptionPreview: transcription.substring(0, 200) + '...'
    });

    // Update UI to show loading state
    console.log('ğŸ”„ Setting loading states');
    setIsGeneratingSummary(true);
    setAiSummary(null); // Clear previous summary
    setError(''); // Clear any errors
    
    // ğŸ” DEBUG: Preparing to generate summary - What we're sending to AI
    console.log('generateSummary called - Summary generation details:', {
      
      // ğŸ¤– AI REQUEST PARAMETERS
      summaryType,                           // String: Type of summary requested ("meeting", "action_items", etc.)
      
      // ğŸ“ INPUT TEXT ANALYSIS
      transcriptionLength: transcription.length,  // Number: How many characters to analyze
      
      // ğŸ’¡ WHAT HAPPENS NEXT:
      // - We'll send this transcription text to our Python backend
      // - Backend forwards it to Google Gemini AI
      // - AI analyzes the text and generates structured summary
      // - We receive back summary with key points, action items, etc.
    });

    try {
      // Send HTTP POST request to backend
      console.log('ğŸ“¤ Sending POST request to /api/summarize');
      console.log('ğŸ“Š Request payload:', {
        textLength: transcription.length,
        summaryType: summaryType,
        endpoint: 'http://localhost:8000/api/summarize'
      });
      
      const response = await fetch('http://localhost:8000/api/summarize', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json', // Tell server we're sending JSON
        },
        body: JSON.stringify({
          text: transcription,        // The transcribed text to analyze
          summary_type: summaryType   // What kind of summary we want
        }),
      });

      console.log('ğŸ“¨ Response received from AI backend - HTTP response details:', {
        
        // ğŸ“Š HTTP STATUS INFORMATION
        status: response.status,               // Number: HTTP status code (200 = success, 400+ = error)
        statusText: response.statusText,       // String: HTTP status message ("OK", "Not Found", etc.)
        ok: response.ok,                       // Boolean: Was the request successful? (status 200-299)
        
        // ğŸ“‹ RESPONSE HEADERS
        headers: Object.fromEntries(response.headers.entries()), // Object: All HTTP headers from server
        
        // ğŸ’¡ WHAT THESE MEAN:
        // - status 200 = Success, AI generated summary
        // - status 400 = Bad request (invalid input)
        // - status 500 = Server error (AI service failed)
        // - ok: true = We can proceed to parse the JSON response
        // - headers contain metadata about the response (content-type, etc.)
      });

      // Check if request was successful
      if (!response.ok) {
        console.log('âŒ Response not OK:', response.status);
        throw new Error(`HTTP error! status: ${response.status}`);
      }

      // Parse the JSON response from backend
      console.log('ğŸ“Š Parsing JSON response');
      const summary: AISummary = await response.json();
      console.log('âœ… Summary received and parsed - AI response analysis:', {
        
        // ğŸ“ SUMMARY CONTENT ANALYSIS
        hasSummary: !!summary.summary,         // Boolean: Did AI generate a main summary?
        hasKeyPoints: !!(summary.key_points && summary.key_points.length > 0),     // Boolean: Are there key points?
        hasActionItems: !!(summary.action_items && summary.action_items.length > 0), // Boolean: Are there action items?
        hasError: !!summary.error,             // Boolean: Did AI encounter an error?
        
        // ğŸ‘€ CONTENT PREVIEW
        summaryPreview: summary.summary?.substring(0, 100) + '...', // String: First 100 chars of summary
        
        // ğŸ’¡ WHAT THIS TELLS US:
        // - hasSummary: true = AI successfully generated main summary text
        // - hasKeyPoints: true = AI extracted important points from the conversation
        // - hasActionItems: true = AI identified tasks or to-dos
        // - hasError: true = Something went wrong with AI processing
        // - summaryPreview gives us a quick look at the generated content
      });
      
      console.log('ğŸ”„ Setting AI summary state');
      setAiSummary(summary); // Update UI with summary

    } catch (error) {
      console.error('âŒ Error generating summary:', error);
      console.log('ğŸ“Š Summary generation error details:', {
        errorName: error instanceof Error ? error.name : 'Unknown',
        errorMessage: error instanceof Error ? error.message : String(error),
        errorStack: error instanceof Error ? error.stack : undefined
      });
      setError('Failed to generate AI summary. Please check if the backend is running.');
    } finally {
      // Always turn off loading state, whether success or failure
      console.log('ğŸ”„ Setting isGeneratingSummary to false');
      setIsGeneratingSummary(false);
    }
  };

  /**
   * ğŸ§¹ CLEAR FUNCTION
   * 
   * Resets the application to initial state
   * Useful for starting a new recording session
   */
  const clearAll = () => {
    console.log('ğŸ§¹ clearAll called - resetting application state');
    console.log('ğŸ“Š State before clearing - What we\'re about to reset:', {
      
      // ğŸ“ TEXT CONTENT TO CLEAR
      transcriptionLength: transcription.length,    // Number: Characters in final transcription
      interimTextLength: interimText.length,        // Number: Characters in live preview
      
      // ğŸ¤– AI CONTENT TO CLEAR
      hasAiSummary: !!aiSummary,                   // Boolean: Do we have AI summary to clear?
      
      // âš ï¸ ERROR STATE TO CLEAR
      hasError: !!error,                           // Boolean: Do we have error message to clear?
      
      // ğŸ’¡ WHAT WILL HAPPEN:
      // - All text content will be emptied (transcription = "")
      // - AI summary will be removed (aiSummary = null)
      // - Error messages will be cleared (error = "")
      // - App returns to initial "ready to record" state
    });
    
    setTranscription('');     // Clear transcribed text
    setInterimText('');       // Clear interim text
    setAiSummary(null);       // Clear AI summary
    setError('');             // Clear error messages
    
    console.log('âœ… All state cleared');
  };

  /**
   * ğŸ¨ RENDER FUNCTION
   * 
   * This is the JSX that defines what the user sees
   * JSX is HTML-like syntax that React converts to DOM elements
   * 
   * STRUCTURE:
   * - Header with app title
   * - Main content area with:
   *   - Text display box (transcription + interim text)
   *   - Control buttons (Start/Stop, Clear)
   *   - Summary section (when available)
   *   - Error display (when needed)
   *   - Status indicator
   */
  
  console.log('ğŸ¨ Rendering UI with current state - What the user will see:', {
    
    // ğŸ¤ RECORDING STATE (affects main button)
    isRecording,                           // Boolean: Controls button text ("Start" vs "Stop")
    
    // ğŸ”— CONNECTION STATE (affects button availability)
    connectionStatus,                      // String: Shown in status area
    
    // ğŸ“ TEXT CONTENT STATE (affects text display)
    transcriptionLength: transcription.length,    // Number: Characters in main text box
    interimTextLength: interimText.length,        // Number: Characters in live preview
    
    // âš ï¸ ERROR STATE (affects error message display)
    hasError: !!error,                     // Boolean: Should we show error message?
    
    // ğŸ¤– AI SUMMARY STATE (affects summary section)
    hasAiSummary: !!aiSummary,            // Boolean: Do we have AI results to show?
    isGeneratingSummary,                   // Boolean: Should we show loading spinner?
    
    // ğŸ›ï¸ UI CONTROL LOGIC (affects button states)
    showSummarySection: !!(aiSummary || isGeneratingSummary),  // Boolean: Show summary section?
    clearButtonDisabled: !transcription && !aiSummary,        // Boolean: Is clear button disabled?
    recordButtonDisabled: connectionStatus === 'Connection Error', // Boolean: Is record button disabled?
    
    // ğŸ’¡ HOW THESE AFFECT THE UI:
    // - isRecording changes button from "Start Recording" to "Stop & Summarize"
    // - connectionStatus shows in status area ("Connected", "Disconnected", etc.)
    // - transcription/interim text fill the main text display box
    // - hasError shows red error message below controls
    // - AI summary states control the summary section visibility and content
    // - Button disabled states prevent user interaction when not ready
  });

  return (
    <div className="simple-app">
      
      {/* ğŸ  SIMPLE HEADER */}
      <div className="simple-header">
        <h1>ğŸ¤ Voice to Text</h1>
      </div>

      {/* ğŸ“± MAIN CONTENT CONTAINER */}
      <div className="simple-main">
        
        {/* ğŸ“ TEXT DISPLAY BOX */}
        {/* This shows the transcribed text and real-time interim text */}
        <div className="simple-textbox">
          
          {/* Final transcribed text - this is the "official" transcription */}
          {transcription && (
            <div className="transcription-text">{transcription}</div>
          )}
          
          {/* Interim text - shows what's being transcribed right now */}
          {/* This gives immediate feedback while speaking */}
          {interimText && (
            <div className="interim-text">{interimText}</div>
          )}
          
          {/* Placeholder text when nothing is transcribed yet */}
          {!transcription && !interimText && (
            <div className="placeholder-text">
              Click "Start" to begin recording...
            </div>
          )}
        </div>

        {/* ğŸ›ï¸ CONTROL BUTTONS */}
        <div className="simple-controls">
          
          {/* MAIN RECORD/STOP BUTTON */}
          {/* This button changes based on recording state */}
          <button
            className={`simple-btn ${isRecording ? 'stop-btn' : 'start-btn'}`}
            onClick={() => {
              console.log('ğŸ–±ï¸ Record/Stop button clicked:', {
                currentState: isRecording ? 'recording' : 'stopped',
                action: isRecording ? 'stopRecording' : 'startRecording'
              });
              return isRecording ? stopRecording() : startRecording();
            }}
            disabled={connectionStatus === 'Connection Error'} // Disable if connection failed
          >
            {isRecording ? (
              // STOP STATE: Show stop icon and "Stop & Summarize" text
              <>
                <MicOff size={20} />
                Stop & Summarize
              </>
            ) : (
              // START STATE: Show microphone icon and "Start Recording" text
              <>
                <Mic size={20} />
                Start Recording
              </>
            )}
          </button>
          
          {/* CLEAR BUTTON */}
          {/* Only enabled when there's content to clear */}
          <button
            className="simple-btn clear-btn"
            onClick={() => {
              console.log('ğŸ–±ï¸ Clear button clicked');
              clearAll();
            }}
            disabled={!transcription && !aiSummary} // Disable if nothing to clear
          >
            Clear
          </button>
        </div>

        {/* ğŸ¤– AI SUMMARY SECTION */}
        {/* Only show this section when generating summary or have results */}
        {(aiSummary || isGeneratingSummary) && (
          <div className="simple-summary">
            <h3>ğŸ“ Summary</h3>
            
            {/* LOADING STATE */}
            {isGeneratingSummary && (
              <div className="loading-text">Generating summary...</div>
            )}
            
            {/* SUMMARY RESULTS */}
            {aiSummary && !isGeneratingSummary && (
              <div className="summary-content">
                
                {/* ERROR HANDLING */}
                {aiSummary.error ? (
                  <div className="error-text">âŒ {aiSummary.error}</div>
                ) : (
                  <>
                    {/* MAIN SUMMARY */}
                    {aiSummary.summary && (
                      <div className="summary-section">
                        <strong>Summary:</strong>
                        <p>{aiSummary.summary}</p>
                      </div>
                    )}
                    
                    {/* KEY POINTS */}
                    {aiSummary.key_points && aiSummary.key_points.length > 0 && (
                      <div className="summary-section">
                        <strong>Key Points:</strong>
                        <ul>
                          {aiSummary.key_points.map((point, index) => (
                            <li key={index}>{point}</li>
                          ))}
                        </ul>
                      </div>
                    )}
                    
                    {/* ACTION ITEMS */}
                    {aiSummary.action_items && aiSummary.action_items.length > 0 && (
                      <div className="summary-section">
                        <strong>Action Items:</strong>
                        <ul>
                          {aiSummary.action_items.map((item, index) => (
                            <li key={index}>
                              {item.task}
                              {item.responsible_party && ` - ${item.responsible_party}`}
                              {item.deadline && ` (Due: ${item.deadline})`}
                            </li>
                          ))}
                        </ul>
                      </div>
                    )}
                  </>
                )}
              </div>
            )}
          </div>
        )}

        {/* âš ï¸ ERROR DISPLAY */}
        {/* Shows error messages when things go wrong */}
        {error && (
          <div className="simple-error">
            âš ï¸ {error}
          </div>
        )}
        
        {/* ğŸ“Š CONNECTION STATUS */}
        {/* Shows current connection status for debugging */}
        <div className="simple-status">
          Status: {connectionStatus}
        </div>
      </div>
    </div>
  );
}

// Export the App component so it can be imported in other files
export default App; 